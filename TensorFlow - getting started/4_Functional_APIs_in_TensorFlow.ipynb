{"cells":[{"cell_type":"markdown","metadata":{"id":"xQvbdV1I0Nwk"},"source":["# __Functional APIs in TensorFlow__\n","\n","\n","The functional APIs in TensorFlow is an alternative way to create and customize complex neural network models. It allows you to build models with more flexibility and handle multiple inputs and outputs."]},{"cell_type":"markdown","metadata":{"id":"M55nCxMGW8Di"},"source":["## Steps to be followed:\n","1. Import the required libraries\n","2. Load the dataset\n","3. Inspect the dataset and visualizing samples\n","4. Build the model\n","5. Compile the model\n","6. Evaluate the model\n"]},{"cell_type":"markdown","metadata":{"id":"UoJfZD0F0Nwv"},"source":["### Step 1: Import the required libraries\n","\n","\n","- Import necessary modules and classes from TensorFlow and other libraries, such as the Functional API components, data preprocessing tools, and visualization utilities.\n","\n","- Set up the basic structure of a neural network model using the Functional API. It defines the input layer, followed by several hidden layers with specific activation functions and dropout regularization. It also specifies the output layer and the optimizer for model training.\n","\n","- Include additional imports and utilities for data handling, such as splitting data into training and testing sets, calculating performance metrics like the confusion matrix, and visualizing images and results. These are useful for data preprocessing, model evaluation, and result visualization."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SO4zPYNm0Nwx","executionInfo":{"status":"ok","timestamp":1719082190946,"user_tz":-330,"elapsed":13368,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}},"outputs":[],"source":["# Import necessary modules and classes from TensorFlow and other libraries\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras import backend as K\n","\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageFont, ImageDraw\n","import itertools\n","import random"]},{"cell_type":"markdown","metadata":{"id":"8OdCs-Jb2uyO"},"source":["### Step 2: Load the dataset\n","- Load the MNIST dataset, which contains handwritten digit images and their labels.\n","- The dataset is divided into training and testing sets, represented by variables **(x_train, y_train)** and **(x_test, y_test)**, respectively.\n","- The pixel values of the images are normalized by dividing them by **255.0** to ensure they are within the range of 0 to 1. This normalization step helps improve the training process."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mGd_NFUs0Nwy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719082206457,"user_tz":-330,"elapsed":1121,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}},"outputId":"96954cec-4b70-4701-f7aa-b2b3d9dfa384"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}],"source":["# Load the MNIST dataset\n","mnist = tf.keras.datasets.mnist\n","\n","# Split the dataset into training and testing sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Normalize the pixel values to be between 0 and 1\n","x_train, x_test = x_train / 255.0, x_test / 255.0"]},{"cell_type":"markdown","metadata":{"id":"OBVKEsFQbHWS"},"source":["### Step 3: Inspect the dataset and visualize the samples\n","- Import the MNIST dataset from the TensorFlow Keras library.\n","- The dimensions of the training and testing data arrays are printed using the shape attribute to show the number of samples and their respective dimensions.\n","- A loop is used to display the first nine images from the training set using Matplotlib.\n","- Each image is shown in a subplot, with the **cmap** parameter set to **gray** to display the images in grayscale."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"QujOUJhU0Nwz","outputId":"db32b70d-def5-425a-bebf-840c48d8a065","executionInfo":{"status":"ok","timestamp":1719082222927,"user_tz":-330,"elapsed":1133,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train: x=(60000, 28, 28), y=(60000,)\n","Test: x=(10000, 28, 28), y=(10000,)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x600 with 9 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAeQAAAHiCAYAAAA597/kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAh+0lEQVR4nO3da5RdZX0/8DMDIdwmw6UUSEMIEO6XRC4KNCuhNYBiDBcLgkAIWsIilYurUqymNBQBwctqJAZUCshlLXQVScRKIZZAkEuE2rhWjNGINRcGSFSSGUJICnP+r/4vzvy2ZOfkzJzfzHw+7/Z3PWefR3yG72zmWc9uqVar1QoA0FStzZ4AAKCQASAFhQwACShkAEhAIQNAAgoZABJQyACQgEIGgAS2LzOou7u70tHRUWlra6u0tLT09pzoZ6rVaqWrq6syfPjwSmtr7/+OZz3yXqxHMtma9ViqkDs6Oir77bdfQybHwLVq1arKiBEjev17rEfKsB7JpMx6LPXrY1tbW0MmxMDWV+vEeqQM65FMyqyTUoXsP8NQRl+tE+uRMqxHMimzTmzqAoAEFDIAJKCQASABhQwACShkAEhAIQNAAgoZABJQyACQgEIGgAQUMgAkoJABIAGFDAAJKGQASEAhA0ACChkAElDIAJDA9s2eANA7jjvuuJB9+tOfDtmUKVNqru+7774w5vbbbw/Zz372s22YHdCTJ2QASEAhA0ACChkAElDIAJCATV09bLfddiFrb2+v+35Fm2h23nnnmutDDz00jPm7v/u7kH3lK18J2QUXXBCyt99+O2Rf+tKXQnbDDTeEjP5p7NixIZs/f37Ihg0bFrJqtVpzffHFF4cxkydPDtmee+65FTOE3vXBD34wZA8++GDIJkyYELJf/epXvTKnreUJGQASUMgAkIBCBoAEFDIAJDAgNnWNHDkyZDvssEPITj755JrrcePGhTG77bZbyD72sY/VP7kSVq9eHbKvf/3rITv77LND1tXVFbKf//znIXv66afrnB3ZvP/97w/Zww8/HLKizYg9N3BVKnENbd68OYwp2sB14oknhqzo9K6i+/GnjR8/PmRF//wfeeSRvphOv3HCCSeE7MUXX2zCTOrnCRkAElDIAJCAQgaABBQyACTQ7zZ1FZ1I9OSTT4ZsW07X6m3d3d011zNmzAhj3nzzzZAVnTrz6quvhuyNN94IWZaTaPjTep7gVqlUKscee2zIHnjggZDtu+++dX/v8uXLa65vu+22MOahhx4K2bPPPhuyorV8yy231D23weiUU04J2cEHHxyywbypq7U1PksecMABIdt///1D1tLS0itzagRPyACQgEIGgAQUMgAkoJABIIF+t6lr5cqVIfvDH/4Qst7e1LVo0aKQrVu3LmR/9Vd/FbKeJxfdf//9DZsX/dc3v/nNkBW9XrPRem4c23XXXcOYopPeijYfHXPMMQ2b12A1ZcqUkD3//PNNmEleRZsYL7vsspAVbYBctmxZr8ypETwhA0ACChkAElDIAJCAQgaABPrdpq4//vGPIbv22mtDNmnSpJD9z//8T8110SsOiyxevDhkp556asg2bNgQsiOPPDJkV199danvZWA77rjjaq4/8pGPhDFlTxUq2nT16KOPhuwrX/lKyDo6Omque/6cVCrFp7/99V//dcgyn4LUXxSdQkWtu+66q9S4nqfQZef/eQBIQCEDQAIKGQASUMgAkEC/29RVZO7cuSEreiVjV1dXzfWYMWPCmE996lMhK9oIU7SBq8gvfvGLkE2bNq3UZxk4il4bOn/+/JrrYcOGhTHVajVkjz32WMiKTvSaMGFCyIpej9hzg8zatWvDmJ///Och6/ka0UqleGNa0Sskf/azn4VssOp5utnee+/dpJn0H2VPYuz5M5adJ2QASEAhA0ACChkAEhgQf0Mu0tnZucUx69evL3WvoreIfPe73w1Z0d/UGHwOOeSQkBUdXtPz72C///3vw5hXX301ZN/5zndC9uabb4bsP/7jP0pljbTTTjuF7O///u9DduGFF/bqPPqTM844o+a66J/hYFb0N/UDDjig1GdfeeWVRk+nV3lCBoAEFDIAJKCQASABhQwACQzYTV1lzJw5M2Q938BTqRQfsDBx4sSQPfHEEw2ZF/3H0KFDQ1Z0kEzPjTuVSjyoZsqUKWHMSy+9FLL+tuln5MiRzZ5CaoceeugWxxQdMDRYFP08FW30+vWvfx2ynj9j2XlCBoAEFDIAJKCQASABhQwACQzqTV1Fb2wqOpWr6M003/72t0O2YMGCkBVtyvnGN75Rc130Rh/6h/e9730hK9rAVeTMM8+suX766acbMicGnhdffLHZU9hmRW8z+9CHPhSyiy66qOb6tNNOK3X/G2+8MWTr1q0rN7kkPCEDQAIKGQASUMgAkIBCBoAEBvWmriIvv/xyyKZOnRqye+65J2QXX3xxqWyXXXapub7vvvvCmKLX7pHP1772tZC1tLSErGjDVn/fxNXaGn+f9wrS3rHHHns09H5jxowJWc91W3Qa4YgRI0K2ww47hKzo9ZpF62Xjxo0hW7RoUc31pk2bwpjtt4/V9d///d8h6288IQNAAgoZABJQyACQgEIGgARs6irhkUceCdny5ctDVrTB54Mf/GDIbr755prr/fffP4y56aabQvbKK6+85zzpXZMmTQrZ2LFjQ1Z08toPfvCD3phSUxVt4Cr637548eI+mE3/1XNjU9E/wzvvvDNkn//85+v+zmOOOSZkPTd1vfPOO2HMW2+9FbKlS5eG7O677w5Z0amFRRsbX3/99Zrr1atXhzFFryBdtmxZyPobT8gAkIBCBoAEFDIAJKCQASABm7rqtGTJkpCdd955IfvoRz8asp6nfF1++eVhzMEHHxyyU089dWumSIMVbSQpOqVozZo1Ifvud7/bK3PqDUOHDg3ZzJkzS332ySefDNk//uM/buuUBrTp06fXXK9YsSKMOfnkkxv6nStXrgzZ3Llza65/+ctfhjEvvPBCQ+dRZNq0aTXXe+21Vxjz29/+ttfn0QyekAEgAYUMAAkoZABIQCEDQAI2dTXQunXrQnb//feH7K677qq5LnqV2Pjx40N2yimnhOypp54qPT/6RtHr4jK/TrPnJq4ZM2aEMddee23Iik5Q+upXvxqyN998cxtmN/jceuutzZ5CUxWdbtjTww8/3Acz6XuekAEgAYUMAAkoZABIwN+Q61T0tpS/+Zu/CdkJJ5wQsqK/GfdU9AaVhQsXlpwdzZT5zU5Fb6fq+ffhj3/842HMvHnzQvaxj32sYfOCrVH0Br6BwBMyACSgkAEgAYUMAAkoZABIwKauHg499NCQffrTnw7ZOeecE7J99tmnru989913Q1Z0kER3d3dd96cxWlpaSmVnnXVWyK6++uremNJ7+sxnPhOyf/qnfwpZe3t7zfWDDz4YxkyZMqVxEwMKeUIGgAQUMgAkoJABIAGFDAAJDKpNXT03XV1wwQVhTNEGrlGjRjV0Hi+99FLN9U033RTGZD7tabCqVqulsqLNfV//+tdDdvfdd9dc/+EPfwhjTjzxxJBdfPHFIRszZkzIRowYEbKVK1eG7PHHH6+5njNnThgDzVK0cfKQQw4J2QsvvNAX0+lVnpABIAGFDAAJKGQASEAhA0ACA2JT19577x2yI444ImSzZ8+uuT7ssMMaOo9FixaF7Mtf/nLIer7KzglcA8t2220XsunTp4es5+sLOzs7w5iDDz647nk899xzIVuwYEHIrr/++rq/A3pb0cbJ1taB+Sw5MP9XAUA/o5ABIAGFDAAJKGQASCD1pq499tgjZN/85jdDNnbs2JAdeOCBDZtH0eaYr371qyHreeJRpVKpbNy4sWHzoLmef/75kL344oshO+GEE0rdr+eJXkWbE4sUnej10EMPhawZr3yEvnDSSSeF7N577+37iTSYJ2QASEAhA0ACChkAElDIAJBAUzZ1feADHwjZtddeG7L3v//9IfuLv/iLhs3jrbfeClnRa/JuvvnmkG3YsKFh86B/WL16dcjOOeeckF1++eUhmzFjRl3fOWvWrJDdcccdIfvNb35T1/0hu6LXLw5UnpABIAGFDAAJKGQASEAhA0ACTdnUdfbZZ5fKylq6dGnIfvjDH4bsnXfeqbkuOm1r3bp1dc+DwefVV18N2cyZM0tlQPTYY4/VXJ977rlNmknf84QMAAkoZABIQCEDQAIt1Wq1uqVBnZ2dlfb29r6YD/3Y+vXrK8OGDev177EeKcN6JJMy69ETMgAkoJABIAGFDAAJKGQASEAhA0ACChkAElDIAJCAQgaABBQyACSgkAEgAYUMAAkoZABIQCEDQAKlCrnEC6Ggz9aJ9UgZ1iOZlFknpQq5q6trmyfDwNdX68R6pAzrkUzKrJNS70Pu7u6udHR0VNra2iotLS0NmRwDR7VarXR1dVWGDx9eaW3t/b+CWI+8F+uRTLZmPZYqZACgd9nUBQAJKGQASEAhA0ACChkAElDIAJCAQgaABBQyACSgkAEgAYUMAAkoZABIQCEDQAIKGQASUMgAkIBCBoAEFDIAJKCQASABhQwACShkAEhAIQNAAgoZABJQyACQgEIGgAQUMgAksH2ZQd3d3ZWOjo5KW1tbpaWlpbfnRD9TrVYrXV1dleHDh1daW3v/dzzrkfdiPZLJ1qzHUoXc0dFR2W+//RoyOQauVatWVUaMGNHr32M9Uob1SCZl1mOpXx/b2toaMiEGtr5aJ9YjZViPZFJmnZQqZP8ZhjL6ap1Yj5RhPZJJmXViUxcAJKCQASABhQwACShkAEhAIQNAAgoZABJQyACQgEIGgAQUMgAkoJABIAGFDAAJKGQASEAhA0ACChkAElDIAJCAQgaABBQyACSgkAEgAYUMAAkoZABIQCEDQALbN3sCVCozZswI2Q033BCy1tb4+9Mpp5wSsqeffroh8wJolLa2tpDtuuuuIfvIRz5Sc73XXnuFMV/72tdCtmnTpm2YXQ6ekAEgAYUMAAkoZABIQCEDQAI2dTXB1KlTa66vu+66MKa7u7vUvarVaiOmBFCXUaNGhazo32knnXRSyI466qi6vnPfffcN2VVXXVXXvTLxhAwACShkAEhAIQNAAgoZABKwqasJ9t9//5rrHXfcsUkzIbsPfOADIbvoootCNmHChJAdeeSRpb7js5/9bM11R0dHGDNu3LiQPfDAAyFbtGhRqe8kv8MOOyxk11xzTcguvPDCkO20004ha2lpCdmqVatC1tXVVXN9+OGHhzHnnXdeyObMmROyZcuWhSwzT8gAkIBCBoAEFDIAJKCQASABm7p62cSJE0N25ZVXbvFzRZsRJk2aFLLXX3+9vomRzsc//vGQzZo1K2R/9md/FrKiDTNPPfVUyIpeZfflL395i3Mrun/Rvc4///wt3ovma29vD9mtt95ac120HoteoVjW8uXLQ3b66aeHbMiQITXXRf8uLPoZKMr6G0/IAJCAQgaABBQyACSgkAEgAZu6GqjoNKN77rknZEUbKnoq2mizYsWK+iZG022/ffxRO/7442uuv/3tb4cxO++8c8gWLlwYshtvvDFkP/nJT0I2dOjQkH3ve9+ruT7ttNPCmCIvvfRSqXHkc/bZZ4fsb//2bxt2/5dffjlkp556asiKTuoaPXp0w+bR33hCBoAEFDIAJKCQASABhQwACdjU1UCXXHJJyIYPH77FzxWdqHTfffc1YkokUfTKxLvuumuLn5s/f37Iik5Q6uzsLDWPos+W2cS1evXqkH3nO98p9Z3kc+6559b1ud/97nche/HFF0N23XXXhaxoA1eRotctDhaekAEgAYUMAAkoZABIwN+Q61T0ZpFPfvKTIevu7g7ZunXraq6/+MUvNmxeNF/RIR2f//znQ1atVmuu58yZE8bMmDEjZGX/XlzkC1/4Ql2fu+qqq0K2du3auudBc1122WUhmzZtWs31E088Ecb85je/CdmaNWsaN7FKpbL33ns39H79iSdkAEhAIQNAAgoZABJQyACQgE1dJYwaNSpkDz/8cN33u/3222uuFyxYUPe9aK7rr78+ZEUbuDZv3hyyxx9/vOa66DCFjRs3lprHjjvuGLKiAz9GjhwZspaWlprrok2G8+bNKzUP+oeOjo6QzZw5s+8nUuCkk05q9hSaxhMyACSgkAEgAYUMAAkoZABIwKauEj70oQ+F7Jhjjin12f/6r/8K2axZs7Z5TvS93XbbLWTTp08PWc8TuCqVuIGrUqlUzjrrrLrmMXr06JA9+OCDITvuuONK3e/f//3fa65vu+22uubF4FR0itsuu+xS9/2OPvroLY557rnnQvb888/X/Z1ZeEIGgAQUMgAkoJABIAGFDAAJ2NTVQ9FGmy996UulPvuTn/wkZJdccknI1q9fv9Xzovl22GGHkBW9hrNI0caXP//zP6+5vvTSS8OYyZMnh+yoo44K2a677hqyos1lRdkDDzxQc71hw4YwhoFv5513rrk+4ogjwph//ud/DtkZZ5xR6v6trfH5r+j1tD0VnSpW9LPy7rvvlppHZp6QASABhQwACShkAEhAIQNAAoN6U1ejX6v429/+NmSvv/563fcjl6JXKK5duzZke+21V8j+93//N2RFG6zKKNrk0tnZGbJ99903ZL///e9D9uijj9Y1D/qHIUOGhOx973tfyHr+u69o/RS9DrRoPRadmlV04mHPjWRFtt8+1tQ555wTsqITEIt+ZjPzhAwACShkAEhAIQNAAgoZABIY1Ju6rrvuupCVOTnmTyl7ohf907p160JWdLLbD3/4w5DtscceIXv55ZdrrufNmxfG3HvvvSH74x//GLKHHnooZEWbcorGMXAUnSZXtJnq+9///hbvdcMNN4TsySefDNmzzz4bsqL1XvTZolPneiraJHnLLbeEbOXKlSGbO3duyDZt2rTF72wWT8gAkIBCBoAEFDIAJKCQASCBQbWpa+zYsTXXp512Wt33KtqA86tf/aru+9E/LVq0KGRFm1Aaafz48SGbMGFCyIo2KBadJkf/VHQCV9FGrGuvvbbU/R577LGa69tvvz2MKdrYWLTef/SjH4Xs6KOPDlnRSVq33XZbzXXRxq8zzzwzZA8++GDIfvzjH4fs1ltvDdkbb7wRsp4WL168xTHbyhMyACSgkAEgAYUMAAkMqr8hP/HEEzXXu+++e6nPvfDCCyGbOnVqI6YEW22nnXYKWdHfi4veJuVgkP5pu+22C9mNN94Yss9+9rMh27BhQ8g+97nPhazn2ij6e/Hxxx8fstmzZ4es6G1Sy5cvD9kVV1wRsgULFtRcDxs2LIw5+eSTQ3bhhReGbPLkySGbP39+yIqsWrWq5vqAAw4o9blt4QkZABJQyACQgEIGgAQUMgAkMKg2de25554112Xf7DRnzpyQvfnmmw2ZE2ytxx9/vNlToI9NmzYtZEUbuN56662QXX755SHrucG1UqlUTjzxxJrrSy+9NIz58Ic/HLKiTYb/8i//ErJ77rknZD03ThXp7OwM2X/+53+Wyi644IKQfeITn9jid1YqlcpnPvOZUuMayRMyACSgkAEgAYUMAAkoZABIoKVadJxPD52dnZX29va+mE/DFG0g6Hm6VtlNXQceeGDIVqxYUde8BrL169cXnqrTaP1xPTbS6aefHrKit+sU/Wjvu+++IVu7dm1jJpbMQFqPr776asiK3rK0adOmkC1btixku+yyS8hGjx5d19xmzpwZsltuuSVk7777bl33HyjKrEdPyACQgEIGgAQUMgAkoJABIIEBcVLX2LFjQzZx4sSQ9dzEtXnz5jDmG9/4Rshef/31+icHDVa0yZCB7bXXXgtZ0aauoUOHhmzMmDGlvqPnxsCFCxeGMXPnzg3Z7373u5AN9g1c9fKEDAAJKGQASEAhA0ACChkAEhgQm7p22223kO2zzz5b/Nwrr7wSsqJXmkEmzzzzTMhaW+Pv1mVPoiO/8ePHh+yss84K2bHHHhuyNWvWhOzuu+8O2RtvvFFzXbTpld7lCRkAElDIAJCAQgaABBQyACQwIDZ1wWCyZMmSkC1fvjxkRSd6HXTQQSEbqK9fHEi6urpCdv/995fK6D88IQNAAgoZABJQyACQgEIGgAQGxKauZcuWhey5554L2bhx4/piOtDnbr755pDdddddIbvppptCduWVV9ZcL126tHETA0rzhAwACShkAEhAIQNAAi3VarW6pUGdnZ2V9vb2vpgP/dj69esrw4YN6/XvsR6jon/u3/ve90I2ceLEkH3/+9+vub700kvDmA0bNmzD7JrDeiSTMuvREzIAJKCQASABhQwACShkAEhgQBwMAoNdZ2dnyM4777yQFR0McsUVV9Rcz5w5M4xxWAj0Pk/IAJCAQgaABBQyACSgkAEgASd10TBORiIT65FMnNQFAP2EQgaABBQyACRQqpBL/JkZ+mydWI+UYT2SSZl1UqqQu7q6tnkyDHx9tU6sR8qwHsmkzDoptcu6u7u70tHRUWlra6u0tLQ0ZHIMHNVqtdLV1VUZPnx4pbW19/8KYj3yXqxHMtma9ViqkAGA3mVTFwAkoJABIAGFDAAJKGQASEAhA0ACChkAElDIAJCAQgaABBQyACSgkAEgAYUMAAkoZABIQCEDQAIKGQASUMgAkIBCBoAEFDIAJKCQASABhQwACShkAEhAIQNAAgoZABJQyACQwPZlBnV3d1c6OjoqbW1tlZaWlt6eE/1MtVqtdHV1VYYPH15pbe393/GsR96L9UgmW7MeSxVyR0dHZb/99mvI5Bi4Vq1aVRkxYkSvf4/1SBnWI5mUWY+lfn1sa2tryIQY2PpqnViPlGE9kkmZdVKqkP1nGMroq3ViPVKG9UgmZdaJTV0AkIBCBoAEFDIAJKCQASABhQwACShkAEhAIQNAAgoZABJQyACQgEIGgAQUMgAkoJABIAGFDAAJKGQASEAhA0ACChkAEti+2RPIZtasWSG76qqrQrZkyZKQTZo0KWQrVqxozMQAGNA8IQNAAgoZABJQyACQgEIGgAQG9aauUaNGheyiiy4KWXd3d8gOP/zwkB122GEhs6mLsg455JCQDRkyJGTjx48P2Zw5c0JWtG4bad68eSE7//zzQ7Z58+ZenQd9p2g9nnzyySG7+eabQ/aXf/mXvTKngcQTMgAkoJABIAGFDAAJKGQASGBQb+pau3ZtyBYuXBiyyZMn98V0GMCOPPLIkE2dOrXm+txzzw1jWlvj78zDhw8PWdEGrmq1uhUz3HpFPxd33nlnyK655pqQdXZ29saU6GXt7e0hW7BgQchee+21kO2zzz6lxg1mnpABIAGFDAAJKGQASEAhA0ACg3pT14YNG0LmZC16wy233BKyM844owkz6V1TpkwJ2b/927+F7Nlnn+2L6dAkRRu4bOraMk/IAJCAQgaABBQyACSgkAEggUG9qWu33XYL2ZgxY/p+Igx48+fPD1mZTV1r1qwJWdEmqaITvcq+frHn6/MmTJhQ6nPwp7S0tDR7Cv2SJ2QASEAhA0ACChkAElDIAJDAoN7UtfPOO4ds5MiRdd/vhBNOCNmyZctC5jSwweeOO+4I2dy5c7f4uf/7v/8LWaNPNxo2bFjN9ZIlS8KYolc+Fin63/TSSy/VNS/6r6JXf+64445NmEn/4gkZABJQyACQgEIGgAQUMgAkMKg3dXV0dITs3nvvDdnMmTNL3a9o3Lp160I2e/bsUvdj4HjnnXdCtmrVqibMJDr99NNrrnffffe677V69eqQbdq0qe77MXAcf/zxIXvhhReaMJO8PCEDQAIKGQASUMgAkMCg/htykRtvvDFkZf+GDNmdf/75Ibvssstqrnfaaae673/99dfX/VnyK9oLsX79+pC1t7eH7KCDDuqVOQ0knpABIAGFDAAJKGQASEAhA0ACNnWV0Noaf2/p7u5uwkyg2IUXXhiyz33ucyEbPXp0yIYMGVLXdy5evDhkRW+nYuAoOujomWeeCdmkSZP6YDYDjydkAEhAIQNAAgoZABJQyACQgE1dJRRt4KpWq02YCf3VqFGjQnbxxRfXXE+cOLHu+48bNy5k9a7Rzs7OkBVtEPvRj34Uso0bN9b1nYAnZABIQSEDQAIKGQASUMgAkIBNXdBgRx11VMh+8IMfhGzkyJF9MZ2tVnTy0re+9a0mzISBZM8992z2FNLzhAwACShkAEhAIQNAAgoZABKwqQv6QEtLS6msXo18RWjRq/M+/OEPh+yxxx6r6/4MTpMnT272FNLzhAwACShkAEhAIQNAAgoZABKwqauEbdkwM378+JDNnj17m+dEXkuWLAnZKaecErKLLrqo5vrxxx8PY95+++2GzatSqVQ+9alPhezKK69s6HcwuCxYsCBkRRsD2TJPyACQgEIGgAQUMgAkoJABIAGbukoo2sBVrVZLffacc84J2RFHHFFzvXTp0vomRr+xYsWKkN100019Po+ZM2eGzKYutsXKlStLjRsyZEjI9t9//5AV/awMFp6QASABhQwACShkAEjA35BLuPPOO0N2+eWX132/adOm1Vxfc801dd8Ltsbpp5/e7CkwwLzzzjulxhW93Wzo0KGNnk6/5gkZABJQyACQgEIGgAQUMgAkYFNXCcuWLWv2FEig6GCD0047LWRPPvlkyDZu3Ngrc3ovl156achmzZrV5/NgYJs3b17Iiv6dedhhh4WsaEPr9OnTGzKv/sgTMgAkoJABIAGFDAAJKGQASKClWuK1RZ2dnZX29va+mE+/8etf/zpkBx10UKnPtrbW/h40evToMObll1+ub2JNtH79+sqwYcN6/Xv6Yj2OGzcuZF/4whdCduqpp4bsgAMOCNmqVasaM7FKpbLHHnuE7IwzzgjZ7bffHrK2trYt3r9oA9rkyZNDtmDBgi3eq5kG0nrsb/71X/81ZEWbDPfee++Qvf32270xpaYrsx49IQNAAgoZABJQyACQgEIGgASc1FWnX/ziFyE78MADS322u7u70dOhwWbPnh2yo446qtRn/+Ef/iFkXV1d2zyn/69oI9mxxx4bshL7NSuVSqXy1FNP1VzfcccdYUz2DVzkV7QeN2/e3ISZ5OUJGQASUMgAkIBCBoAEFDIAJGBTV52+9a1vheyjH/1oE2ZCNldccUWzp1CpVCqVNWvWhOzRRx8N2dVXX11zPVBPSqK5ik6pOvPMM0P2yCOP9MV0UvKEDAAJKGQASEAhA0ACChkAErCpq05Lly4N2S9/+cuQHX744X0xHRps6tSpIbvyyitDdskll/TqPIpew/nWW2+F7JlnnglZ0cbDJUuWNGZi8B7OO++8kG3atClkRf/OHMw8IQNAAgoZABJQyACQgEIGgARs6qrTihUrQnb00Uc3YSb0hsWLF4ds+vTpIfvpT38asi9+8Ysh23333UM2d+7cmuv58+eHMfPmzQvZa6+9FjLIZOHChSEr2uC6cePGvphOv+EJGQASUMgAkIBCBoAEWqrVanVLgzo7Oyvt7e19MR/6sfXr1xe+0aXRrEfKsB7JpMx69IQMAAkoZABIQCEDQAIKGQASUMgAkIBCBoAEFDIAJKCQASABhQwACShkAEhAIQNAAgoZABJQyACQQKlCLvFCKOizdWI9Uob1SCZl1kmpQu7q6trmyTDw9dU6sR4pw3okkzLrpNT7kLu7uysdHR2Vtra2SktLS0Mmx8BRrVYrXV1dleHDh1daW3v/ryDWI+/FeiSTrVmPpQoZAOhdNnUBQAIKGQASUMgAkIBCBoAEFDIAJKCQASABhQwACfw//Ouycl9EdToAAAAASUVORK5CYII=\n"},"metadata":{}}],"source":["# Print the shape of the training and testing data\n","print('Train: x=%s, y=%s' % (x_train.shape, y_train.shape))\n","print('Test: x=%s, y=%s' % (x_test.shape, y_test.shape))\n","\n","# Display the first nine images from the training set\n","plt.figure(figsize=(6,6))\n","for i in range(9):\n","    plt.subplot(330 + 1 + i)\n","    plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ud1PiE8DcCHU"},"source":["  **Observations:**\n","  - The output **Train: x=(60000, 28, 28), y=(60000,)** indicates that the training dataset consists of 60,000 samples with dimensions of 28x28 pixels. The input images are grayscale, meaning they have only one color channel.\n","  - The output **Test: x=(10000, 28, 28), y=(10000,)** indicates that the testing dataset contains 10,000 samples with the same dimensions as the training dataset.\n","  - The **x** values represent the input images, and their shape is the number of samples, height, width. The **y** values represent the corresponding labels for the images, indicating the correct digit or class associated with each image."]},{"cell_type":"markdown","metadata":{"id":"37xXRl9aaYhg"},"source":["### Step 4: Build the model\n","- The model consists of an input layer, a flatten layer, a dense layer with ReLU activation, and an output layer with softMax activation.\n","- The model is then returned. Calling **model.summary()** will display a summary of the model's architecture, including the shapes of the input and output tensors and the number of parameters in each layer."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"wami7nO40Nw2","executionInfo":{"status":"ok","timestamp":1719082302043,"user_tz":-330,"elapsed":441,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}},"outputs":[],"source":["# Function to build the neural network model using the Functional API\n","def build_model_wtih_functional():\n","    input_layer = tf.keras.Input(shape=(28, 28))\n","    flatten_layer = tf.keras.layers.Flatten()(input_layer)\n","    first_dense = tf.keras.layers.Dense(128, activation=tf.nn.relu)(flatten_layer)\n","    output_layer = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(first_dense)\n","\n","    func_model = Model(inputs=input_layer, outputs=output_layer)\n","    return func_model\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_ZEdrAe0Nw4","outputId":"cf3ac84a-0944-428c-a557-574999818dd1","executionInfo":{"status":"ok","timestamp":1719082342117,"user_tz":-330,"elapsed":427,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 28, 28)]          0         \n","                                                                 \n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," dense (Dense)               (None, 128)               100480    \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                1290      \n","                                                                 \n","=================================================================\n","Total params: 101770 (397.54 KB)\n","Trainable params: 101770 (397.54 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Build the model\n","model = build_model_wtih_functional()\n","\n","# Display the model architecture\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"VFCi-Kp5byeJ"},"source":["**Observations:**\n","\n","- The model architecture consists of an input layer, a flatten layer, a dense layer with ReLU activation, and an output layer with softMax activation.\n","- The flatten layer is used to transform the 2D input images into a 1D array, while the dense layers perform the actual computations and transformations.\n","- The model has a total of **101,770** trainable parameters, which are learned during the training process. These parameters include the weights and biases of the dense layers."]},{"cell_type":"markdown","metadata":{"id":"q3Qv46O8ck9w"},"source":["### Step 5: Compile the model\n","- Compile the model using the Adam optimizer, sparse categorical cross-entropy loss, and accuracy as the metric.\n","- The model is then trained for five epochs using the training data **x_train** and labels **y_train**, aiming to minimize the loss and improve accuracy."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"flOBZszj0Nw7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"66f7b9d4-ab54-47a2-8fd7-202b76ada5d1","executionInfo":{"status":"ok","timestamp":1719082432949,"user_tz":-330,"elapsed":32687,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 [==============================] - 7s 3ms/step - loss: 0.2578 - accuracy: 0.9264\n","Epoch 2/5\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.1135 - accuracy: 0.9669\n","Epoch 3/5\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0779 - accuracy: 0.9761\n","Epoch 4/5\n","1875/1875 [==============================] - 5s 3ms/step - loss: 0.0575 - accuracy: 0.9823\n","Epoch 5/5\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0446 - accuracy: 0.9858\n","{'loss': [0.2578033208847046, 0.11350174248218536, 0.07788512110710144, 0.05750098079442978, 0.044587597250938416], 'accuracy': [0.9264166951179504, 0.966866672039032, 0.9761166572570801, 0.9823166728019714, 0.9857833385467529]}\n"]}],"source":["# Compile the model with the Adam optimizer, sparse categorical cross-entropy loss, and accuracy metric\n","model.compile(optimizer=tf.optimizers.Adam(),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model with the training data for 5 epochs\n","history = model.fit(x_train, y_train, epochs=5)\n","print(history.history)"]},{"cell_type":"markdown","metadata":{"id":"OfIlPkIzytTt"},"source":["**Observations:**\n","- The training output shows the progress of the model's training over five epochs, with each epoch representing a complete pass through the training dataset.\n","- The output includes the values of the loss and accuracy metrics for each epoch, indicating how well the model is learning from the training data.\n","- The displayed metrics gradually improve as the model trains, with the loss decreasing and the accuracy increasing."]},{"cell_type":"markdown","metadata":{"id":"544kWNUIy779"},"source":["### Step 6: Evaluate the model\n","- The **model.evaluate(x_test, y_test)** is used to evaluate the trained model on the test dataset.\n","- It computes the loss value and metrics (such as accuracy) of the model's performance on the test data."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ieRFOPZN0Nw_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c53332a-8c84-462d-a01c-b2df3ee326cf","executionInfo":{"status":"ok","timestamp":1719082444523,"user_tz":-330,"elapsed":1203,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9786\n","Test Loss: 0.07186905294656754\n","Test Accuracy: 0.978600025177002\n"]}],"source":["# Evaluate the trained model on the test dataset\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","print('Test Loss:', test_loss)\n","print('Test Accuracy:', test_acc)"]},{"cell_type":"markdown","metadata":{"id":"GS7gZx5XzRyX"},"source":["**Observations:**\n","\n","- The first number **0.07** represents the average loss or error of the model's predictions compared to the actual results on the test data. A lower value indicates better accuracy.\n","- The second number ~**0.98** represents the accuracy of the model based on the test data. It shows the proportion of correctly predicted results out of all the test samples. A higher value indicates better performance."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}